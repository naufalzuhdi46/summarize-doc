# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/18pBk5XSya0fM-HzhA1c1k9Asyp0wLlT8
"""

# wrap the output in colab cells
from IPython.display import HTML, display

def set_css():
  display(HTML('''

  '''))
get_ipython().events.register('pre_run_cell', set_css)

# install transformers with sentencepiece
!pip install transformers[sentencepiece]

file = open("/content/cobain.txt", "r")
FileContent = file.read().strip()

# display file content
FileContent

# total characters in the file
len(FileContent)

# import and initialize the tokenizer and model from the checkpoint
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

checkpoint = "sshleifer/distilbart-cnn-12-6"

tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint)

# max tokens including the special tokens
tokenizer.model_max_length

# max tokens excluding the special tokens
tokenizer.max_len_single_sentence

# number of special tokens
tokenizer.num_special_tokens_to_add()

# extract the sentences from the document
import nltk
nltk.download('punkt')
sentences = nltk.tokenize.sent_tokenize(FileContent)

# find the max tokens in the longest sentence
max([len(tokenizer.tokenize(sentence)) for sentence in sentences])

# initialize
length = 0
chunk = ""
chunks = []
count = -1
for sentence in sentences:
  count += 1
  combined_length = len(tokenizer.tokenize(sentence)) + length # add the no. of sentence tokens to the length counter

  if combined_length  <= tokenizer.max_len_single_sentence: # if it doesn't exceed
    chunk += sentence + " " # add the sentence to the chunk
    length = combined_length # update the length counter

    # if it is the last sentence
    if count == len(sentences) - 1:
      chunks.append(chunk.strip()) # save the chunk

  else:
    chunks.append(chunk.strip()) # save the chunk

    # reset
    length = 0
    chunk = ""

    # take care of the overflow sentence
    chunk += sentence + " "
    length = len(tokenizer.tokenize(sentence))
len(chunks)

[len(tokenizer.tokenize(c)) for c in chunks]

[len(tokenizer(c).input_ids) for c in chunks]

sum([len(tokenizer(c).input_ids) for c in chunks])

len(tokenizer(FileContent).input_ids)

sum([len(tokenizer.tokenize(c)) for c in chunks])

len(tokenizer.tokenize(FileContent))

# inputs to the model
inputs = [tokenizer(chunk, return_tensors="pt") for chunk in chunks]

for input in inputs:
  output = model.generate(**input)
  print(tokenizer.decode(*output, skip_special_tokens=True))